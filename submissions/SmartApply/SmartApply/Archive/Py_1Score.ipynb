{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raviteja R\n",
      "Senior Product Data Analyst, \n",
      "Certified Data Scientist\n",
      "r2raviteja@gmail.com\n",
      " \n",
      "+49 15216 442399\n",
      " \n",
      "Munich, Germany\n",
      " \n",
      "https://www.linkedin.com/in/rraviteja/\n",
      " \n",
      "Profile  with 5 years of experience in \n",
      "analytics, A/B testing and KPI optimization. \n",
      "Proficient in building scalable dashboards \n",
      "and machine learning models to drive \n",
      "business impact. Available for immediate \n",
      "joining.\n",
      "Education\n",
      "Masters in Business Analytics & Data \n",
      "Science, EU Business School\n",
      "March 2023 – April 2025 | Munich, Germany\n",
      "GPA : 1,6/4 (German Scale,  1 = Highest)\n",
      "Post Graduate Diploma in Data Science, \n",
      "IIIT Bangalore\n",
      "April 2019 – September 2020\n",
      "GPA - 3.2/4 (U.S Scale, with 4 = Highest)\n",
      "Bachelor of Technology (ECE) | \n",
      "Specialization in Data Science, \n",
      "Lovely Professional University\n",
      "June 2015 – April 2019\n",
      "GPA : 7.76/10 (with 10 = Highest)\n",
      "Projects\n",
      "Thesis: AI-Driven Interview Bot\n",
      "Developed an AI-powered interview bot \n",
      "leveraging Generative AI, Langchain, RAG \n",
      "and NLP, reducing early-stage interviewer \n",
      "workload by 60% and increasing candidate \n",
      "satisfaction by 16%.\n",
      "Data Scientist - case study\n",
      "Developed a Logistic Regression model \n",
      "(83% precision, 81% recall, 80% F1-score) \n",
      "to predict high-traffic recipes, boosting user \n",
      "engagement and traffic by 15% through \n",
      "personalized meal plans and targeted \n",
      "content.\n",
      "Skills\n",
      "Programming\n",
      "Python, SQL, ML, NLP\n",
      "Data Analysis\n",
      "A/B Testing, Statistics, Pandas, EDA, SKlearn, Excel\n",
      "Visualization\n",
      "PowerBi, Metabase, Tableau, Streamlit\n",
      "GenAI & Other\n",
      "LLMs, AI Agents, Chatbots, RAG, LangChain, LangGraph, LLMOps, \n",
      "AWS, Git, Jira, Salesforce\n",
      "Professional Experience\n",
      "Data Analyst (werkstudent), ALLIANZ PARTNERS\n",
      "November 2023 – present | Munich, Germany\n",
      "•Developed Power BI dashboards for C-level executives across \n",
      "119 business units, LOBs, optimizing insights into a $2.8B \n",
      "contract portfolio. Automated reporting processes, reducing \n",
      "manual Excel workload by 60%.\n",
      "•Developed Python scripts to process unstructured Salesforce and \n",
      "finance data, optimizing data load time and efficiency.\n",
      "Senior Product Data Analyst in Customer Experience, RAPIDO\n",
      "April 2021 – February 2023 | Bangalore, India\n",
      "•Increased key KPI's: Rides per Customer (RPC) by 5% and \n",
      "Customer Satisfaction Score (CSAT) by 22%.\n",
      "•Designed and executed A/B test experiments on Scratch Card, \n",
      "drop suggestions, Home Favorite and single-click booking and \n",
      "built dashboards using SQL & Metabase.\n",
      "•Consolidated 15 event-level datasets into a master dataset, \n",
      "reducing week on week dashboard load times from 15 minutes to \n",
      "under 30 seconds.\n",
      "•Performed NLP-driven sentiment analysis on customer feedback, \n",
      "increasing CSAT from 2.8 to 3.9 and boosting cashless payment \n",
      "adoption by 8%.\n",
      "•Specialized in product analytics, leveraging funnel conversion \n",
      "tracking, user retention analysis, UTM tracking, affiliate marketing \n",
      "platforms, and digital ad performance (CTR, CPC, CPA).\n",
      "Associate Software Engineer, TCS & Ucodesoft Solutions\n",
      "February 2019 – March 2021 | India\n",
      "Ucodesoft - Developed a user-friendly frontend website for a \n",
      "restaurant using html, css, php, JavaScript. enhancing online \n",
      "customer experience with features such as online menus and \n",
      "reservations.\n",
      "TCS - Automated AWS VDI control using Python, reducing manual \n",
      "effort by 60%.\n",
      "Developed a real-time hand detection system for Unilever with \n",
      "90% accuracy, enabling automated machine shutdowns to prevent \n",
      "factory injuries.\n",
      "Certificates AND Languages\n",
      "Certified Data Scientist, Datacamp\n",
      "Languages\n",
      "English\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f339fdaf13465192ff654aaf82afc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 85.0\n",
      "Reason: The candidate possesses strong related experience with data analysis, dashboards, SQL, Python, and cross-team collaboration, aligning well with the JD requirements. However, explicit experience with major HRIS platforms like ADP or Workday isn't clearly documented, which slightly reduces the perfect match score.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ec975b2556424391371ee32f3261de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "\n",
    "import pandas as pd\n",
    "def score(num_job):\n",
    "    with open(\"JD.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df[\"Combined\"].iloc[num_job])\n",
    "\n",
    "    def load_file(file_path: str):\n",
    "        \"\"\"\n",
    "        Load content from a PDF, Word, or text file.\n",
    "        \"\"\"\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a .pdf, .docx, or .txt file.\")\n",
    "        documents = loader.load()\n",
    "        return documents#\"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "\n",
    "    # Paths to the job description and resume files\n",
    "    jd = \"JD.txt\"  # Replace with your file path\n",
    "    cv = \"CV.pdf\"  # Replace with your file path\n",
    "\n",
    "    def load_cv_jd(jd1= None, cv1= None):\n",
    "\n",
    "        # Load the files\n",
    "        try:\n",
    "            job_description = load_file(jd1)\n",
    "            resume1 = load_file(cv1)\n",
    "            \n",
    "            rjd = \"\\n\".join([doc.page_content for doc in job_description])\n",
    "            rcv = \"\\n\".join([doc.page_content for doc in resume1])\n",
    "            return rjd, rcv\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "    JD, CV = load_cv_jd(jd,cv)\n",
    "    print(CV)\n",
    "\n",
    "    from agno.agent import Agent\n",
    "    from agno.models.openai import OpenAIChat\n",
    "    from agno.tools.reasoning import ReasoningTools\n",
    "\n",
    "    resume_match_scorer_agent = Agent(\n",
    "        name=\"Resume Match Scorer\",\n",
    "        role=\"Evaluate how well a given resume (CV) matches a provided Job Description (JD)\",\n",
    "        model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "        tools=[ReasoningTools(add_instructions=True)],\n",
    "        instructions=[\n",
    "            \"Compare the provided Resume (CV) and Job Description (JD).\",\n",
    "            \"Analyze relevant aspects like skills, experience, education, certifications, and job requirements.\",\n",
    "            \"Score the match between 0 and 100, where 100 means 'perfect match' and 0 means 'no match'.\",\n",
    "            \"Consider how closely the resume's skills and experience align with the JD requirements.\",\n",
    "            \"If important skills or experiences are missing, reduce the score proportionally.\",\n",
    "            \"Return the output strictly as a JSON object with the following format: {\\\"match_score\\\": float, \\\"reasoning\\\": string}\",\n",
    "            \"In 'reasoning', briefly explain why the score was given (2-4 lines).\",\n",
    "            \"Do not add any extra text outside the JSON (no ```json block or commentary).\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "    )\n",
    "\n",
    "    query = \" Compare both  Resume : [ \" + CV + \" ], Job description [\" +JD+ \" ] \"\n",
    "    response = resume_match_scorer_agent.run(query)\n",
    "    resume_match_scorer_agent.print_response(response.content)\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Assuming llmresponse.content is your model output\n",
    "    raw_output = response.content.strip()\n",
    "\n",
    "    # Try parsing as JSON\n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "        score = parsed.get(\"match_score\")\n",
    "        reason = parsed.get(\"reasoning\")\n",
    "        print(\"Score:\", score)\n",
    "        print(\"Reason:\", reason)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON.\")\n",
    "\n",
    "    email_extractor_agent = Agent(\n",
    "        name=\"Email Extractor\",\n",
    "        role=\"Extract email addresses from job descriptions\",\n",
    "        model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "        tools=[ReasoningTools(add_instructions=True)],\n",
    "        instructions=[\n",
    "            \"Extract any email addresses found in the provided text.\",\n",
    "            \"Return ONLY the email address if found.\",\n",
    "            \"If no email is found, return an empty string.\",\n",
    "            \"Do not include any additional text, JSON formatting, or explanations.\",\n",
    "        ],\n",
    "        markdown=False,  # disable Markdown formatting\n",
    "    )\n",
    "    # Test the email extractor\n",
    "    query_email = \"Extract email from this text: [\" + JD + \"]\"  # Using CV instead of JD since it contains an email\n",
    "    response_email = email_extractor_agent.run(query_email)\n",
    "    print(response_email.content.strip())  # strip() removes any extra whitespace\n",
    "    recruiter_email = response_email.content\n",
    "\n",
    "    # Update only the 9th row\n",
    "    df.at[num_job, \"score\"] = score\n",
    "    df.at[num_job, \"reason\"] = reason\n",
    "    df.at[num_job, \"recruiter_email\"] = recruiter_email\n",
    "\n",
    "    \"\"\"\n",
    "    # Read the content from CL.txt\n",
    "    with open('CL.txt', 'r') as file:\n",
    "        cl_content = file.read()\n",
    "\n",
    "    # Assign the content to the DataFrame cell\n",
    "    df.at[8, \"Coverletter\"] = cl_content\n",
    "    \"\"\"\n",
    "\n",
    "    language_finder_agent = Agent(\n",
    "        name=\"Language Finder\",\n",
    "        role=\"Check if a text contains only English and no mention of other languages.\",\n",
    "        model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "        tools=[ReasoningTools(add_instructions=True)],\n",
    "        instructions=[\n",
    "            \"Read the input text carefully.\",\n",
    "            \"If any word refers to another language (like 'German', 'French', 'Spanish', etc.), even if written in English, return score 0.\",\n",
    "            \"Only return score 1 if the text has no mention of any other language and is fully in English.\",\n",
    "            \"Ignore context. Just the presence of any other language name means score 0.\",\n",
    "            \"Return JSON only: {\\\"language_score\\\": int, \\\"reason\\\": string}\",\n",
    "            \"In 'reasoning', clearly say which word caused score 0.\",\n",
    "            \"No extra text or formatting outside the JSON.\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "    )\n",
    "    query = \"Analyze this text for non-English content: [\" + JD + \"]\"\n",
    "    response = language_finder_agent.run(query)\n",
    "    language_finder_agent.print_response(response.content)\n",
    "\n",
    "    # Assuming llmresponse.content is your model output\n",
    "    raw_output = response.content.strip()\n",
    "\n",
    "    # Try parsing as JSON\n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "        score = parsed.get(\"language_score\")\n",
    "        reason = parsed.get(\"reason\")\n",
    "        #print(\"Score:\", score)\n",
    "        #print(\"Reason:\", reason)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON.\")\n",
    "\n",
    "    if score==0:\n",
    "        print(1)\n",
    "        df.at[num_job, \"score\"] = score\n",
    "        df.at[num_job, \"reason\"] = reason\n",
    "\n",
    "    df.to_csv(\"JobsData.csv\",index=False)\n",
    "    df2 = df[['Title', 'Company', 'Location', \"score\",\"reason\",\"recruiter_email\", 'Description',\"Coverletter\"]]\n",
    "    df2 = df2.replace(',', '', regex=True)\n",
    "    df2.to_csv(\"JobsData2.csv\",index=False)\n",
    "\n",
    "df = pd.read_csv(r\"JobsData.csv\")\n",
    "num_job = len(df)\n",
    "#for i in range(2):\n",
    "score(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
