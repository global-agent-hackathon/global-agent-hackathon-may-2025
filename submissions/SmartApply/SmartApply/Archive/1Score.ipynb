{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raviteja R\n",
      "Senior Product Data Analyst, \n",
      "Certified Data Scientist\n",
      "r7raviteja@gmail.com\n",
      " \n",
      "+49 15216 442399\n",
      " \n",
      "Munich, Germany\n",
      " \n",
      "https://www.linkedin.com/in/rraviteja/\n",
      " \n",
      "Profile  with 5 years of experience in \n",
      "analytics, A/B testing and KPI optimization. \n",
      "Proficient in building scalable dashboards \n",
      "and machine learning models to drive \n",
      "business impact. Available for immediate \n",
      "joining.\n",
      "Education\n",
      "Masters in Business Analytics & Data \n",
      "Science, EU Business School\n",
      "March 2023 – April 2025 | Munich, Germany\n",
      "GPA : 1,6/4 (German Scale,  1 = Highest)\n",
      "Post Graduate Diploma in Data Science, \n",
      "IIIT Bangalore\n",
      "April 2019 – September 2020\n",
      "GPA - 3.2/4 (U.S Scale, with 4 = Highest)\n",
      "Bachelor of Technology (ECE) | \n",
      "Specialization in Data Science, \n",
      "Lovely Professional University\n",
      "June 2015 – April 2019\n",
      "GPA : 7.76/10 (with 10 = Highest)\n",
      "Projects\n",
      "Thesis: AI-Driven Interview Bot\n",
      "Developed an AI-powered interview bot \n",
      "leveraging Generative AI, Langchain, RAG \n",
      "and NLP, reducing early-stage interviewer \n",
      "workload by 60% and increasing candidate \n",
      "satisfaction by 16%.\n",
      "Data Scientist - case study\n",
      "Developed a Logistic Regression model \n",
      "(83% precision, 81% recall, 80% F1-score) \n",
      "to predict high-traffic recipes, boosting user \n",
      "engagement and traffic by 15% through \n",
      "personalized meal plans and targeted \n",
      "content.\n",
      "Skills\n",
      "Programming\n",
      "Python, SQL, ML, NLP\n",
      "Data Analysis\n",
      "A/B Testing, Statistics, Pandas, EDA, SKlearn, Excel\n",
      "Visualization\n",
      "PowerBi, Metabase, Tableau, Streamlit\n",
      "GenAI & Other\n",
      "LLMs, AI Agents, Chatbots, RAG, LangChain, LangGraph, LLMOps, \n",
      "AWS, Git, Jira, Salesforce\n",
      "Professional Experience\n",
      "Data Analyst (werkstudent), ALLIANZ PARTNERS\n",
      "November 2023 – present | Munich, Germany\n",
      "•Developed Power BI dashboards for C-level executives across \n",
      "119 business units, LOBs, optimizing insights into a $2.8B \n",
      "contract portfolio. Automated reporting processes, reducing \n",
      "manual Excel workload by 60%.\n",
      "•Developed Python scripts to process unstructured Salesforce and \n",
      "finance data, optimizing data load time and efficiency.\n",
      "Senior Product Data Analyst in Customer Experience, RAPIDO\n",
      "April 2021 – February 2023 | Bangalore, India\n",
      "•Increased key KPI's: Rides per Customer (RPC) by 5% and \n",
      "Customer Satisfaction Score (CSAT) by 22%.\n",
      "•Designed and executed A/B test experiments on Scratch Card, \n",
      "drop suggestions, Home Favorite and single-click booking and \n",
      "built dashboards using SQL & Metabase.\n",
      "•Consolidated 15 event-level datasets into a master dataset, \n",
      "reducing week on week dashboard load times from 15 minutes to \n",
      "under 30 seconds.\n",
      "•Performed NLP-driven sentiment analysis on customer feedback, \n",
      "increasing CSAT from 2.8 to 3.9 and boosting cashless payment \n",
      "adoption by 8%.\n",
      "•Specialized in product analytics, leveraging funnel conversion \n",
      "tracking, user retention analysis, UTM tracking, affiliate marketing \n",
      "platforms, and digital ad performance (CTR, CPC, CPA).\n",
      "Associate Software Engineer, TCS & Ucodesoft Solutions\n",
      "February 2019 – March 2021 | India\n",
      "Ucodesoft - Developed a user-friendly frontend website for a \n",
      "restaurant using html, css, php, JavaScript. enhancing online \n",
      "customer experience with features such as online menus and \n",
      "reservations.\n",
      "TCS - Automated AWS VDI control using Python, reducing manual \n",
      "effort by 60%.\n",
      "Developed a real-time hand detection system for Unilever with \n",
      "90% accuracy, enabling automated machine shutdowns to prevent \n",
      "factory injuries.\n",
      "Certificates AND Languages\n",
      "Certified Data Scientist, Datacamp\n",
      "Languages\n",
      "English\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d53d44866e4d0f93ba45fca78f2bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 85\n",
      "Reason: The candidate's resume demonstrates strong alignment with the key technical skills required for the role, including PowerBI, SQL, Python, and stakeholder management. While not explicitly mentioning Snowflake or web analytics platforms, the overall experience and skills closely match the job's core requirements, resulting in a high compatibility score.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\960ra\\AppData\\Local\\Temp\\ipykernel_9752\\2722233404.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[num_job, \"recruiter_email\"] = recruiter_email\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a602660341541aeb90e2eaafc4b20fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raviteja R\n",
      "Senior Product Data Analyst, \n",
      "Certified Data Scientist\n",
      "r7raviteja@gmail.com\n",
      " \n",
      "+49 15216 442399\n",
      " \n",
      "Munich, Germany\n",
      " \n",
      "https://www.linkedin.com/in/rraviteja/\n",
      " \n",
      "Profile  with 5 years of experience in \n",
      "analytics, A/B testing and KPI optimization. \n",
      "Proficient in building scalable dashboards \n",
      "and machine learning models to drive \n",
      "business impact. Available for immediate \n",
      "joining.\n",
      "Education\n",
      "Masters in Business Analytics & Data \n",
      "Science, EU Business School\n",
      "March 2023 – April 2025 | Munich, Germany\n",
      "GPA : 1,6/4 (German Scale,  1 = Highest)\n",
      "Post Graduate Diploma in Data Science, \n",
      "IIIT Bangalore\n",
      "April 2019 – September 2020\n",
      "GPA - 3.2/4 (U.S Scale, with 4 = Highest)\n",
      "Bachelor of Technology (ECE) | \n",
      "Specialization in Data Science, \n",
      "Lovely Professional University\n",
      "June 2015 – April 2019\n",
      "GPA : 7.76/10 (with 10 = Highest)\n",
      "Projects\n",
      "Thesis: AI-Driven Interview Bot\n",
      "Developed an AI-powered interview bot \n",
      "leveraging Generative AI, Langchain, RAG \n",
      "and NLP, reducing early-stage interviewer \n",
      "workload by 60% and increasing candidate \n",
      "satisfaction by 16%.\n",
      "Data Scientist - case study\n",
      "Developed a Logistic Regression model \n",
      "(83% precision, 81% recall, 80% F1-score) \n",
      "to predict high-traffic recipes, boosting user \n",
      "engagement and traffic by 15% through \n",
      "personalized meal plans and targeted \n",
      "content.\n",
      "Skills\n",
      "Programming\n",
      "Python, SQL, ML, NLP\n",
      "Data Analysis\n",
      "A/B Testing, Statistics, Pandas, EDA, SKlearn, Excel\n",
      "Visualization\n",
      "PowerBi, Metabase, Tableau, Streamlit\n",
      "GenAI & Other\n",
      "LLMs, AI Agents, Chatbots, RAG, LangChain, LangGraph, LLMOps, \n",
      "AWS, Git, Jira, Salesforce\n",
      "Professional Experience\n",
      "Data Analyst (werkstudent), ALLIANZ PARTNERS\n",
      "November 2023 – present | Munich, Germany\n",
      "•Developed Power BI dashboards for C-level executives across \n",
      "119 business units, LOBs, optimizing insights into a $2.8B \n",
      "contract portfolio. Automated reporting processes, reducing \n",
      "manual Excel workload by 60%.\n",
      "•Developed Python scripts to process unstructured Salesforce and \n",
      "finance data, optimizing data load time and efficiency.\n",
      "Senior Product Data Analyst in Customer Experience, RAPIDO\n",
      "April 2021 – February 2023 | Bangalore, India\n",
      "•Increased key KPI's: Rides per Customer (RPC) by 5% and \n",
      "Customer Satisfaction Score (CSAT) by 22%.\n",
      "•Designed and executed A/B test experiments on Scratch Card, \n",
      "drop suggestions, Home Favorite and single-click booking and \n",
      "built dashboards using SQL & Metabase.\n",
      "•Consolidated 15 event-level datasets into a master dataset, \n",
      "reducing week on week dashboard load times from 15 minutes to \n",
      "under 30 seconds.\n",
      "•Performed NLP-driven sentiment analysis on customer feedback, \n",
      "increasing CSAT from 2.8 to 3.9 and boosting cashless payment \n",
      "adoption by 8%.\n",
      "•Specialized in product analytics, leveraging funnel conversion \n",
      "tracking, user retention analysis, UTM tracking, affiliate marketing \n",
      "platforms, and digital ad performance (CTR, CPC, CPA).\n",
      "Associate Software Engineer, TCS & Ucodesoft Solutions\n",
      "February 2019 – March 2021 | India\n",
      "Ucodesoft - Developed a user-friendly frontend website for a \n",
      "restaurant using html, css, php, JavaScript. enhancing online \n",
      "customer experience with features such as online menus and \n",
      "reservations.\n",
      "TCS - Automated AWS VDI control using Python, reducing manual \n",
      "effort by 60%.\n",
      "Developed a real-time hand detection system for Unilever with \n",
      "90% accuracy, enabling automated machine shutdowns to prevent \n",
      "factory injuries.\n",
      "Certificates AND Languages\n",
      "Certified Data Scientist, Datacamp\n",
      "Languages\n",
      "English\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40549a91b98044fd9dfa242b82f99502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 75\n",
      "Reason: The candidate has strong data analytics, visualization, and machine learning skills, with relevant project experience that align well with the role. However, there is limited demonstrated experience with specific analytics engineering tools like dbt, BigQuery, or Looker, which are important for the role. Their customer focus and cross-functional collaboration add value, but missing some key technical tools reduces the score.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a4d7ee441e488bb829c061a3c0aa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "\n",
    "import pandas as pd\n",
    "def score(num_job):\n",
    "    with open(\"JD.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df[\"Combined\"].iloc[num_job])\n",
    "\n",
    "    def load_file(file_path: str):\n",
    "        \"\"\"\n",
    "        Load content from a PDF, Word, or text file.\n",
    "        \"\"\"\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a .pdf, .docx, or .txt file.\")\n",
    "        documents = loader.load()\n",
    "        return documents#\"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "\n",
    "    # Paths to the job description and resume files\n",
    "    jd = \"JD.txt\"  # Replace with your file path\n",
    "    cv = \"CV.pdf\"  # Replace with your file path\n",
    "\n",
    "    def load_cv_jd(jd1= None, cv1= None):\n",
    "\n",
    "        # Load the files\n",
    "        try:\n",
    "            job_description = load_file(jd1)\n",
    "            resume1 = load_file(cv1)\n",
    "            \n",
    "            rjd = \"\\n\".join([doc.page_content for doc in job_description])\n",
    "            rcv = \"\\n\".join([doc.page_content for doc in resume1])\n",
    "            return rjd, rcv\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    JD, CV = load_cv_jd(jd,cv)\n",
    "    print(CV)\n",
    "\n",
    "    from agno.agent import Agent\n",
    "    from agno.models.openai import OpenAIChat\n",
    "    from agno.tools.reasoning import ReasoningTools\n",
    "\n",
    "    resume_match_scorer_agent = Agent(\n",
    "        name=\"Resume Match Scorer\",\n",
    "        role=\"Evaluate how well a given resume (CV) matches a provided Job Description (JD)\",\n",
    "        model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "        tools=[ReasoningTools(add_instructions=True)],\n",
    "        instructions=[\n",
    "            \"Compare the provided Resume (CV) and Job Description (JD).\",\n",
    "            \"Analyze relevant aspects like skills, experience, education, certifications, and job requirements.\",\n",
    "            \"Score the match between 0 and 100, where 100 means 'perfect match' and 0 means 'no match'.\",\n",
    "            \"Consider how closely the resume's skills and experience align with the JD requirements.\",\n",
    "            \"If important skills or experiences are missing, reduce the score proportionally.\",\n",
    "            \"Return the output strictly as a JSON object with the following format: {\\\"match_score\\\": float, \\\"reasoning\\\": string}\",\n",
    "            \"In 'reasoning', briefly explain why the score was given (2-4 lines).\",\n",
    "            \"Do not add any extra text outside the JSON (no ```json block or commentary).\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "    )\n",
    "\n",
    "    query = \" Compare both  Resume : [ \" + CV + \" ], Job description [\" +JD+ \" ] \"\n",
    "    response = resume_match_scorer_agent.run(query)\n",
    "    resume_match_scorer_agent.print_response(response.content)\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Assuming llmresponse.content is your model output\n",
    "    raw_output = response.content.strip()\n",
    "\n",
    "    # Try parsing as JSON\n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "        score = parsed.get(\"match_score\")\n",
    "        reason = parsed.get(\"reasoning\")\n",
    "        print(\"Score:\", score)\n",
    "        print(\"Reason:\", reason)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON.\")\n",
    "\n",
    "    email_extractor_agent = Agent(\n",
    "        name=\"Email Extractor\",\n",
    "        role=\"Extract email addresses from job descriptions\",\n",
    "        model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "        tools=[ReasoningTools(add_instructions=True)],\n",
    "        instructions=[\n",
    "            \"Extract any email addresses found in the provided text.\",\n",
    "            \"Return ONLY the email address if found.\",\n",
    "            \"If no email is found, return an empty string.\",\n",
    "            \"Do not include any additional text, JSON formatting, or explanations.\",\n",
    "        ],\n",
    "        markdown=False,  # disable Markdown formatting\n",
    "    )\n",
    "    # Test the email extractor\n",
    "    query_email = \"Extract email from this text: [\" + JD + \"]\"  # Using CV instead of JD since it contains an email\n",
    "    response_email = email_extractor_agent.run(query_email)\n",
    "    print(response_email.content.strip())  # strip() removes any extra whitespace\n",
    "    recruiter_email = response_email.content\n",
    "\n",
    "    # Update only the 9th row\n",
    "    df.at[num_job, \"score\"] = score\n",
    "    df.at[num_job, \"reason\"] = reason\n",
    "    df.at[num_job, \"recruiter_email\"] = recruiter_email\n",
    "\n",
    "    \"\"\"\n",
    "    # Read the content from CL.txt\n",
    "    with open('CL.txt', 'r') as file:\n",
    "        cl_content = file.read()\n",
    "\n",
    "    # Assign the content to the DataFrame cell\n",
    "    df.at[8, \"Coverletter\"] = cl_content\n",
    "    \"\"\"\n",
    "\n",
    "    language_finder_agent = Agent(\n",
    "        name=\"Language Finder\",\n",
    "        role=\"Check if a text contains only English and no mention of other languages.\",\n",
    "        model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "        tools=[ReasoningTools(add_instructions=True)],\n",
    "        instructions=[\n",
    "            \"Read the input text carefully.\",\n",
    "            \"If any word refers to another language (like 'German', 'French', 'Spanish', etc.), even if written in English, return score 0.\",\n",
    "            \"Only return score 1 if the text has no mention of any other language and is fully in English.\",\n",
    "            \"Ignore context. Just the presence of any other language name means score 0.\",\n",
    "            \"Return JSON only: {\\\"language_score\\\": int, \\\"reason\\\": string}\",\n",
    "            \"In 'reasoning', clearly say which word caused score 0.\",\n",
    "            \"No extra text or formatting outside the JSON.\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "    )\n",
    "    query = \"Analyze this text for non-English content: [\" + JD + \"]\"\n",
    "    response = language_finder_agent.run(query)\n",
    "    language_finder_agent.print_response(response.content)\n",
    "\n",
    "    # Assuming llmresponse.content is your model output\n",
    "    raw_output = response.content.strip()\n",
    "\n",
    "    # Try parsing as JSON\n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "        score = parsed.get(\"language_score\")\n",
    "        reason = parsed.get(\"reason\")\n",
    "        #print(\"Score:\", score)\n",
    "        #print(\"Reason:\", reason)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON.\")\n",
    "\n",
    "    if score==0:\n",
    "        print(1)\n",
    "        df.at[num_job, \"score\"] = score\n",
    "        df.at[num_job, \"reason\"] = reason\n",
    "\n",
    "    df.to_csv(\"JobsData.csv\",index=False)\n",
    "    df2 = df[['Title', 'Company', 'Location', \"score\",\"reason\",\"recruiter_email\", 'Description',\"Coverletter\"]]\n",
    "    df2 = df2.replace(',', '', regex=True)\n",
    "    df2.to_csv(\"JobsData2.csv\",index=False)\n",
    "\n",
    "df = pd.read_csv(r\"JobsData.csv\")\n",
    "num_job = len(df)\n",
    "for i in range(2):\n",
    "    score(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"JobsData.csv\")\n",
    "num_job = 2\n",
    "with open(\"JD.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df[\"Combined\"].iloc[num_job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raviteja R\n",
      "Senior Product Data Analyst, \n",
      "Certified Data Scientist\n",
      "r7raviteja@gmail.com\n",
      " \n",
      "+49 15216 442399\n",
      " \n",
      "Munich, Germany\n",
      " \n",
      "https://www.linkedin.com/in/rraviteja/\n",
      " \n",
      "Profile  with 5 years of experience in \n",
      "analytics, A/B testing and KPI optimization. \n",
      "Proficient in building scalable dashboards \n",
      "and machine learning models to drive \n",
      "business impact. Available for immediate \n",
      "joining.\n",
      "Education\n",
      "Masters in Business Analytics & Data \n",
      "Science, EU Business School\n",
      "March 2023 – April 2025 | Munich, Germany\n",
      "GPA : 1,6/4 (German Scale,  1 = Highest)\n",
      "Post Graduate Diploma in Data Science, \n",
      "IIIT Bangalore\n",
      "April 2019 – September 2020\n",
      "GPA - 3.2/4 (U.S Scale, with 4 = Highest)\n",
      "Bachelor of Technology (ECE) | \n",
      "Specialization in Data Science, \n",
      "Lovely Professional University\n",
      "June 2015 – April 2019\n",
      "GPA : 7.76/10 (with 10 = Highest)\n",
      "Projects\n",
      "Thesis: AI-Driven Interview Bot\n",
      "Developed an AI-powered interview bot \n",
      "leveraging Generative AI, Langchain, RAG \n",
      "and NLP, reducing early-stage interviewer \n",
      "workload by 60% and increasing candidate \n",
      "satisfaction by 16%.\n",
      "Data Scientist - case study\n",
      "Developed a Logistic Regression model \n",
      "(83% precision, 81% recall, 80% F1-score) \n",
      "to predict high-traffic recipes, boosting user \n",
      "engagement and traffic by 15% through \n",
      "personalized meal plans and targeted \n",
      "content.\n",
      "Skills\n",
      "Programming\n",
      "Python, SQL, ML, NLP\n",
      "Data Analysis\n",
      "A/B Testing, Statistics, Pandas, EDA, SKlearn, Excel\n",
      "Visualization\n",
      "PowerBi, Metabase, Tableau, Streamlit\n",
      "GenAI & Other\n",
      "LLMs, AI Agents, Chatbots, RAG, LangChain, LangGraph, LLMOps, \n",
      "AWS, Git, Jira, Salesforce\n",
      "Professional Experience\n",
      "Data Analyst (werkstudent), ALLIANZ PARTNERS\n",
      "November 2023 – present | Munich, Germany\n",
      "•Developed Power BI dashboards for C-level executives across \n",
      "119 business units, LOBs, optimizing insights into a $2.8B \n",
      "contract portfolio. Automated reporting processes, reducing \n",
      "manual Excel workload by 60%.\n",
      "•Developed Python scripts to process unstructured Salesforce and \n",
      "finance data, optimizing data load time and efficiency.\n",
      "Senior Product Data Analyst in Customer Experience, RAPIDO\n",
      "April 2021 – February 2023 | Bangalore, India\n",
      "•Increased key KPI's: Rides per Customer (RPC) by 5% and \n",
      "Customer Satisfaction Score (CSAT) by 22%.\n",
      "•Designed and executed A/B test experiments on Scratch Card, \n",
      "drop suggestions, Home Favorite and single-click booking and \n",
      "built dashboards using SQL & Metabase.\n",
      "•Consolidated 15 event-level datasets into a master dataset, \n",
      "reducing week on week dashboard load times from 15 minutes to \n",
      "under 30 seconds.\n",
      "•Performed NLP-driven sentiment analysis on customer feedback, \n",
      "increasing CSAT from 2.8 to 3.9 and boosting cashless payment \n",
      "adoption by 8%.\n",
      "•Specialized in product analytics, leveraging funnel conversion \n",
      "tracking, user retention analysis, UTM tracking, affiliate marketing \n",
      "platforms, and digital ad performance (CTR, CPC, CPA).\n",
      "Associate Software Engineer, TCS & Ucodesoft Solutions\n",
      "February 2019 – March 2021 | India\n",
      "Ucodesoft - Developed a user-friendly frontend website for a \n",
      "restaurant using html, css, php, JavaScript. enhancing online \n",
      "customer experience with features such as online menus and \n",
      "reservations.\n",
      "TCS - Automated AWS VDI control using Python, reducing manual \n",
      "effort by 60%.\n",
      "Developed a real-time hand detection system for Unilever with \n",
      "90% accuracy, enabling automated machine shutdowns to prevent \n",
      "factory injuries.\n",
      "Certificates AND Languages\n",
      "Certified Data Scientist, Datacamp\n",
      "Languages\n",
      "English\n"
     ]
    }
   ],
   "source": [
    "def load_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Load content from a PDF, Word, or text file.\n",
    "    \"\"\"\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .pdf, .docx, or .txt file.\")\n",
    "    documents = loader.load()\n",
    "    return documents#\"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "\n",
    "# Paths to the job description and resume files\n",
    "jd = \"JD.txt\"  # Replace with your file path\n",
    "cv = \"CV.pdf\"  # Replace with your file path\n",
    "\n",
    "def load_cv_jd(jd1= None, cv1= None):\n",
    "\n",
    "    # Load the files\n",
    "    try:\n",
    "        job_description = load_file(jd1)\n",
    "        resume1 = load_file(cv1)\n",
    "        \n",
    "        rjd = \"\\n\".join([doc.page_content for doc in job_description])\n",
    "        rcv = \"\\n\".join([doc.page_content for doc in resume1])\n",
    "        return rjd, rcv\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "JD, CV = load_cv_jd(jd,cv)\n",
    "print(CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"title: Senior Full Stack Data Analyst\\ncompany: Pleo\\nlocation: Anywhere\\ndescription: We're searching for a talented and experienced Senior Full Stack Data Analyst to join our growing analytics team. You’ll be partnering closely with product teams across Pleo to unlock value from our rich data to build great products and drive growth. Your data expertise & insight generations will directly influence key product decisions, driving significant growth and improving the experience of our customers.\\n\\nYou will thrive in this role if you:\\n• Are an experienced data practitioner with a dual background in analytics and analytics engineering within a fast-paced environment, preferably fintech\\n• Possess expert technical skills and experience with tools such as advanced SQL, dbt, BigQuery, Looker (and LookML)\\n• Have a customer-centric approach to solving business problems, using data to improve customer experience\\n• Are passionate about building high-quality, robust, and user-friendly dashboards and tools for diverse stakeholders\\n• Like to be involved in both engineering excellent products and understanding business and product challenges with analytics.\\n• Demonstrate a strong sense of ownership and responsibility for your work, with ability to manage and deliver on multiple projects with close attention to detail\\n• Are highly familiar with CI/CD processes and understand their importance in maintaining data integrity\\n\\nWhat will you be doing:\\n• Building end-to-end analytics solutions, using data modelling, analytics techniques and data visualisation to power Pleo\\n• Conduct in-depth analysis of user behavior and product performance data to identify key trends, opportunities, and risks, and translate those findings into actionable insights\\n• Design, implement, and analyze A/B tests and other experiments to optimize product features and improve user engagement, ensuring statistical rigor and validity\\n• Identify and deliver scalable methods of providing self-serve product analytics empowering product teams to make data-driven decisions that lead to faster iteration cycles and improved product outcomes\\n• Contribute to identification, development and delivery of data-driven products\\n• Engaging with data colleagues and peers to foster a culture of cross-team learning and knowledge sharing\\n\\nYour colleagues say you\\n• Are an effective bridge-builder who creates strong partnerships across data, engineering, product, and business teams\\n• Balance innovation and pragmatism, knowing when to go deep and when to deliver quickly\\n• Are a trusted mentor who uplifts and develops those around you\\n• Actively seek and share knowledge, contributing to a culture of continuous learning\\n• Have a knack for bringing people together and aligning them towards shared goals\\n• Are someone they’d love to have coffee with (virtual or not)\\n\\nShow me the benefits!\\n• Your own Pleo card (no more out-of-pocket spending!)\\n• Lunch is on us - with catering in our Lisbon, and London offices, or a monthly lunch allowance paid directly together with your salary in other markets 🍜\\n• Private health insurance to ensure you’re fit in body and mind to do your best work\\n• We offer 25 days of holiday + your public holidays\\n• Flexibility/remote working options\\n• Option to purchase 5 additional days of holiday through a salary sacrifice\\n• We’re trialling MyndUp to give our employees access to free mental health and wellbeing support with great success so far ❤️🩹\\n• Access to LinkedIn Learning - acquire new skills, stay abreast of industry trends and fuel your personal and professional development continuously\\n• Paid parental leave - we want to make sure that we're supportive of families and help you feel that you don't have to compromise your family due to work 👶\\n\\nWhy join us?\\n\\nWorking at Pleo means you're working on something very exciting: the future of work. Our mission is to help every company go beyond the books. Pleo itself means ‘more than you’d expect’, and it’s been the secret to our success over the last 8 years. So it’s only fitting that we’d pass this philosophy onto our customers to help them make the most of their finances.\\n\\nWe think company spending should be delegated to all employees and teams, that it should be as automated as possible, and that it should drive a culture of responsible spending. Finance teams shouldn’t be siloed from the rest of the organisation – they should work in unity with marketing, sales, IT and everyone else.\\n\\nSpeaking of working in unity, our values tell the story of how we work at Pleo. We have four core values, the first of which is ‘champion the customer’, which means we address real pain points that businesses face. Next up is ‘succeed as a team’, which highlights how our strength lies in our diversity and trust in each other. We also ‘make it happen’ by taking bold decisions and following through to deliver results. Last but not least, we ‘build to scale’, creating lasting solutions that address today’s challenges and anticipate tomorrow’s needs.\\n\\nSo, in a nutshell, that's Pleo. Today we are a 850+ team, from over 100 nations, sitting in our Copenhagen HQ, London, Stockholm, Berlin, Madrid, Montreal and Lisbon offices —and quite a few full-time remotes in 35 other countries! Being HQ'd out of Copenhagen means we're inspired by things like a good work-life balance. If you don't work in the office with us, we'll help you set up the best remote setup possible and make sure you still have time to connect with your team.\\n\\nAbout Your Application\\n• Please submit your application in English; it’s our company language so you’ll be speaking lots of it if you join 💕\\n• We treat all candidates equally: If you are interested please apply through our application system - any correspondence should come from there! Our lovely support isn't able to pass on any calls/ emails our way - and this makes sure that the candidate experience is smooth and fair to everyone 😊\\n• We’re on a mission to make everyone feel valued at work. That’s only achievable if our team reflects the diversity of the world around us - and that starts with you, hitting apply, even if you are worried you might not tick all the boxes! We embrace and encourage people from all backgrounds to apply - regardless of race/ethnicity, colour, religion, nationality, gender, sex, sexual orientation, age, marital status, disability, neurodiversity, socio-economic status, culture or beliefs\\n• When you submit an application we process your personal data as a data processor. Find out more about how your data is used in the FAQs section at the bottom of our jobs page\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037688e3c4a644c2b565ff3d193a3ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "from agno.tools.reasoning import ReasoningTools\n",
    "\n",
    "resume_match_scorer_agent = Agent(\n",
    "    name=\"Resume Match Scorer\",\n",
    "    role=\"Evaluate how well a given resume (CV) matches a provided Job Description (JD)\",\n",
    "    model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "    tools=[ReasoningTools(add_instructions=True)],\n",
    "    instructions=[\n",
    "        \"Compare the provided Resume (CV) and Job Description (JD).\",\n",
    "        \"Analyze relevant aspects like skills, experience, education, certifications, and job requirements.\",\n",
    "        \"Score the match between 0 and 100, where 100 means 'perfect match' and 0 means 'no match'.\",\n",
    "        \"Consider how closely the resume's skills and experience align with the JD requirements.\",\n",
    "        \"If important skills or experiences are missing, reduce the score proportionally.\",\n",
    "        \"Return the output strictly as a JSON object with the following format: {\\\"match_score\\\": float, \\\"reasoning\\\": string}\",\n",
    "        \"In 'reasoning', briefly explain why the score was given (2-4 lines).\",\n",
    "        \"Do not add any extra text outside the JSON (no ```json block or commentary).\",\n",
    "    ],\n",
    "    markdown=True,\n",
    ")\n",
    "\n",
    "query = \" Compare both  Resume : [ \" + CV + \" ], Job description [\" +JD+ \" ] \"\n",
    "response = resume_match_scorer_agent.run(query)\n",
    "resume_match_scorer_agent.print_response(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 85\n",
      "Reason: The candidate demonstrates strong data analysis skills, experience with Power BI, SQL, Python, and customer-centric analytics. Their background aligns well with the role's requirements for handling large datasets, customer segmentation, and translating data into actionable insights. Slightly more specific experience in weekly customer analysis and segmentation would improve the fit further.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming llmresponse.content is your model output\n",
    "raw_output = response.content.strip()\n",
    "\n",
    "# Try parsing as JSON\n",
    "try:\n",
    "    parsed = json.loads(raw_output)\n",
    "    score = parsed.get(\"match_score\")\n",
    "    reason = parsed.get(\"reasoning\")\n",
    "    print(\"Score:\", score)\n",
    "    print(\"Reason:\", reason)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "email_extractor_agent = Agent(\n",
    "    name=\"Email Extractor\",\n",
    "    role=\"Extract email addresses from job descriptions\",\n",
    "    model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "    tools=[ReasoningTools(add_instructions=True)],\n",
    "    instructions=[\n",
    "        \"Extract any email addresses found in the provided text.\",\n",
    "        \"Return ONLY the email address if found.\",\n",
    "        \"If no email is found, return an empty string.\",\n",
    "        \"Do not include any additional text, JSON formatting, or explanations.\",\n",
    "    ],\n",
    "    markdown=False,  # disable Markdown formatting\n",
    ")\n",
    "# Test the email extractor\n",
    "query_email = \"Extract email from this text: [\" + JD + \"]\"  # Using CV instead of JD since it contains an email\n",
    "response_email = email_extractor_agent.run(query_email)\n",
    "print(response_email.content.strip())  # strip() removes any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruiter_email = response_email.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\960ra\\AppData\\Local\\Temp\\ipykernel_17684\\259406231.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[num_job, \"recruiter_email\"] = recruiter_email\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Read the content from CL.txt\\nwith open(\\'CL.txt\\', \\'r\\') as file:\\n    cl_content = file.read()\\n\\n# Assign the content to the DataFrame cell\\ndf.at[8, \"Coverletter\"] = cl_content\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update only the 9th row\n",
    "df.at[num_job, \"score\"] = score\n",
    "df.at[num_job, \"reason\"] = reason\n",
    "df.at[num_job, \"recruiter_email\"] = recruiter_email\n",
    "\n",
    "\"\"\"\n",
    "# Read the content from CL.txt\n",
    "with open('CL.txt', 'r') as file:\n",
    "    cl_content = file.read()\n",
    "\n",
    "# Assign the content to the DataFrame cell\n",
    "df.at[8, \"Coverletter\"] = cl_content\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741031eb19934620bd1cf3d768d9a3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "language_finder_agent = Agent(\n",
    "    name=\"Language Finder\",\n",
    "    role=\"Check if a text contains only English and no mention of other languages.\",\n",
    "    model=OpenAIChat(\"gpt-4.1-nano\"),\n",
    "    tools=[ReasoningTools(add_instructions=True)],\n",
    "    instructions=[\n",
    "        \"Read the input text carefully.\",\n",
    "        \"If any word refers to another language (like 'German', 'French', 'Spanish', etc.), even if written in English, return score 0.\",\n",
    "        \"Only return score 1 if the text has no mention of any other language and is fully in English.\",\n",
    "        \"Ignore context. Just the presence of any other language name means score 0.\",\n",
    "        \"Return JSON only: {\\\"language_score\\\": int, \\\"reason\\\": string}\",\n",
    "        \"In 'reasoning', clearly say which word caused score 0.\",\n",
    "        \"No extra text or formatting outside the JSON.\",\n",
    "    ],\n",
    "    markdown=True,\n",
    ")\n",
    "query = \"Analyze this text for non-English content: [\" + JD + \"]\"\n",
    "response = language_finder_agent.run(query)\n",
    "language_finder_agent.print_response(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming llmresponse.content is your model output\n",
    "raw_output = response.content.strip()\n",
    "\n",
    "# Try parsing as JSON\n",
    "try:\n",
    "    parsed = json.loads(raw_output)\n",
    "    score = parsed.get(\"language_score\")\n",
    "    reason = parsed.get(\"reason\")\n",
    "    #print(\"Score:\", score)\n",
    "    #print(\"Reason:\", reason)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse JSON.\")\n",
    "\n",
    "if score==0:\n",
    "    print(1)\n",
    "    df.at[num_job, \"score\"] = score\n",
    "    df.at[num_job, \"reason\"] = reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"JobsData.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['Title', 'Company', 'Location', \"score\",\"reason\",\"recruiter_email\", 'Description',\"Coverletter\"]]\n",
    "df2 = df2.replace(',', '', regex=True)\n",
    "df2.to_csv(\"JobsData2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
